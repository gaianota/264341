{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First step is to import useful libraries, that will be necessary for the next step of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sms = pd.read_csv('social_media_shares.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We divide data in X and y, where X contains the independent variables, the regressor ones, that help us in predicting the response variable, that is stored in y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X = sms.iloc[:,:-1]\n",
    "y = sms.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We proceed with the Explanatory data analysis (EDA) with visualization, to better understand the relationship between the variables and to understand our dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use these function to generate a descriptive statistics that summarize the central tendency, dispersion and shape of our dataset’s distribution.\n",
    "We want also to see how our dataset is structured in terms of the type of information (numerical,categorical,empty values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(sms.describe())\n",
    "print(sms.head())\n",
    "print(sms.info())\n",
    "print('Social media share dataset shape is', sms.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want also to see the type of correlation between independent and dependent variable, so we use scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#We plot the relation between each regressor variable with the response variable \"shares\"\n",
    "for col in X.columns:\n",
    "  sns.scatterplot(data=sms,x=col,y='shares')\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there is not a linear correlation , but actually we can notice something else from these plots.In fact some plots are actually very similar, this is maybe because there is some similiarity are correlation between the regressor variables, so check our assumption we plot the correlation matrix thaks to heat map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "corr = sms.corr()\n",
    "plt.figure(figsize = (15,8)) #To set the figure size\n",
    "sns.heatmap(data=corr, square=True, annot=True, cbar=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can notice that there is a high correlation (almost 0.99) between some variables, meaning that they give a similiary, almost the same information, so we decide to remove and leave only one of the two.\n",
    "We do this by selecting the upper triangle of the correlation matrix and remove the colum having a correlation value bigger than 0,95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "upper_tri = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool_))\n",
    "to_drop = list([column for column in upper_tri.columns if any(upper_tri[column] > 0.95)])\n",
    "X = X.drop(to_drop, axis=1)\n",
    "# Let's check the new shape of our X matrix, containing our predictor variables:\n",
    "# X is now with 56 variables because we removed the useless ones\n",
    "print('New shape of the X matrix is ',X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We now proceed in generating a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#Let's see the distribution of our data in the three splits\n",
    "print(\"X:train shape: \", X_train.shape)\n",
    "print(\"X:test shape: \", X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is feature engineering, so we extract features from raw data, we check for categorical data to manage, remove outliers and so on.\n",
    "As we saw from the function info we do not have categorical data but we have some variables that are binary (0,1) they look like as one hot coder was already made on them, so we will not scale them.\n",
    "To avoid scalinge the wrong columns we select their indexes and we proceed with Robust Scaler on all the other columns. \n",
    "We used Robust Scalar because it's efficient both for scaling and for the removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#We do this in order to avoid scaling columns that have just a binary value (0,1),(as if the One Hot Encoder has done)\n",
    "#We save the index of the columns to now which columns need to\n",
    "index_k = X.columns.get_loc('keywords')\n",
    "index_w = X.columns.get_loc('world')\n",
    "index_m = X.columns.get_loc('monday')\n",
    "index_is = X.columns.get_loc('is_weekend')\n",
    "index_len = len(X.columns)\n",
    "\n",
    "pipeline = ColumnTransformer([\n",
    "  ('num', RobustScaler(), list(range(0, index_k + 1))),\n",
    "  ('num1', RobustScaler(), list(range(index_w + 1, index_m))),\n",
    "  ('num2', RobustScaler(), list(range(index_is, index_len))),\n",
    "\n",
    "], remainder='passthrough')\n",
    "sc = StandardScaler()\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test = pipeline.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know start implementing the regression models, we want to implement Random Forest Regressor, SVR and Polynomial Regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to create a function that gives us the best choices for the hyperparameters.\n",
    "We proceed with a cross validation function in order to select the best combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#GRID SEARCH with cross validation\n",
    "#esplor the parameters to find the best combination\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "\n",
    "def best_scores_grid_search_df(reg):\n",
    "    reg.cv_results_\n",
    "    grid_table = pd.DataFrame(reg.cv_results_)\n",
    "    colums_wanted = ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "    grid_table_rank = grid_table[colums_wanted].sort_values(by='rank_test_score', ascending=True)\n",
    "    return grid_table_rank"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test all different combination of the hyperparameters we create a validation test, that is a small portion of the train test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.18, random_state=42)\n",
    "print(\"X:train shape: \", X_train.shape)\n",
    "print(\"X:valid shape: \", X_valid.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with SVM, let's find which combination of hyperparameters gives a lower error ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "#create regressor object\n",
    "svr =  SVR()\n",
    "#select the hyperparameters\n",
    "param_grid_svr = {'kernel': ['poly', 'rbf', 'sigmoid'], 'C': [np.power (10., a) for a in range(-1,3)],'gamma' : ['scale', 'auto']}\n",
    "reg_svr = GridSearchCV(svr, param_grid_svr, cv=10,scoring='neg_mean_absolute_error')\n",
    "reg_svr.fit(X_valid,y_valid)\n",
    "# we print our greed to see the best combination\n",
    "SVM_best_scores=best_scores_grid_search_df(reg_svr).head()\n",
    "print(SVM_best_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Now that we have found the optimal combination of the parameters we used them for defining the regressor that we will use after."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we use these hyperparameters for the regression\n",
    "svr_best_params = SVM_best_scores.best_params_\n",
    "svr_regression  = SVR(kernel=svr_best_params['kernel'],C = svr_best_params['C'],gamma= svr_best_params['gamma'])\n",
    "#our final regressor with the correct parameters is svr_regressor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know proceed with Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# create regressor object\n",
    "regressor_rf = RandomForestRegressor()\n",
    "#select the hyperparameters\n",
    "param_grid_rf ={'max_depth': range(4, 7),'n_estimators': (100, 300, 500)}\n",
    "rfr_grid = GridSearchCV(regressor_rf,param_grid_rf,cv=10,scoring='neg_mean_absolute_error', verbose=0, n_jobs=-1)\n",
    "rfr_grid_result = rfr_grid.fit(X_valid, y_valid)\n",
    "# we print our greed to see the best combination\n",
    "rfr_best_scores=best_scores_grid_search_df(rfr_grid).head()\n",
    "print(rfr_best_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same process for Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfr_best_params = rfr_grid_result.best_params_\n",
    "rfr = RandomForestRegressor(max_depth=rfr_best_params[\"max_depth\"], n_estimators=rfr_best_params[\"n_estimators\"],random_state=False, verbose=False)\n",
    "#our final regressor with the correct parameters is rfr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed find the best combination of hyperparameters for Polynomial Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#Create regressor object\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "#select the hyperparameters\n",
    "param_grid_poly = {'polynomialfeatures__degree': np.arange(5), 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]}\n",
    "\n",
    "poly_grid = GridSearchCV(PolynomialRegression(), param_grid_poly, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "poly_grid_result = poly_grid.fit(X_valid, y_valid)\n",
    "# we print our greed to see the best combination\n",
    "poly_best_scores=best_scores_grid_search_df(poly_grid).head()\n",
    "print(poly_best_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Same process for Polynomial Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#our final regressor with the correct parameters is poly_regressor\n",
    "poly_best_params = poly_grid_result.best_params_\n",
    "poly_regressor = PolynomialRegression('polynomialfeatures__degree' = poly_best_params['polynomialfeatures__degree'], 'linearregression__fit_intercept'=poly_best_params['linearregression__fit_intercept'],'linearregression__normalize'=['linearregression__normalize'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Now we train our train data with our regression model with the optimal hyperparameters and then we measure it performance on the test set, checking the mean square error, relative error and absolute error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Starting with th SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svr_regression.fit(X_train,y_train)\n",
    "y_pred = svr_regression.predict(X_test)\n",
    "print(mean_squared_error(y_valid,y_pred,squared=False))\n",
    "print(mean_absolute_error(y_valid, y_pred))\n",
    "print(mean_absolute_percentage_error(y_valid, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We proceed with Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rfr.fit(X_train,y_train)\n",
    "y_pred = rfr.predict(X_test)\n",
    "print(mean_squared_error(y_valid,y_pred,squared=False))\n",
    "print(mean_absolute_error(y_valid, y_pred))\n",
    "print(mean_absolute_percentage_error(y_valid, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly Polynomial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poly_regressor.fit(X_train,y_train)\n",
    "y_pred = poly_regressor.predict(X_test)\n",
    "print(mean_squared_error(y_valid,y_pred,squared=False))\n",
    "print(mean_absolute_error(y_valid, y_pred))\n",
    "print(mean_absolute_percentage_error(y_valid, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the final task of the project we select a subset of the attributes(only the most relevant) and with it we train and test again our best model.\n",
    "Since our best model is SVM we will try the subset of attributes only on it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From our analysis the most relevant columns are:\n",
    "\n",
    "● tokens_title: Number of words in the title 0\n",
    "● tokens_content: Number of words in the content 1\n",
    "● hrefs: Number of links 5\n",
    "● imgs: Number of images 7\n",
    "● videos: Number of videos 8\n",
    "● lifestyle: Topic Lifestyle 11\n",
    "● entertainment: Topic Entertainment 12\n",
    "● bus: Topic Business 13\n",
    "● socmed: Topic Social Media 14\n",
    "● tech: Topic Tech 15\n",
    "● world: Topic World 16\n",
    "● self_reference_min_shares: Min. shares of referenced articles in Mashable 26\n",
    "● self_reference_max_shares: Max. shares of referenced articles in Mashable 27\n",
    "● self_reference_avg_shares: Avg. shares of referenced articles in Mashable 28\n",
    "● global_sentiment_polarity: Text sentiment polarity 43\n",
    "● global_rate_positive_words: Rate of positive words in the content 44\n",
    "● global_rate_negative_words: Rate of negative words in the content 45\n",
    "● title_sentiment_polarity: Title polarity 55\n",
    "● abs_title_subjectivity: Absolute subjectivity level 56\n",
    "● abs_title_sentiment_polarity: Absolute polarity level 57"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a new X test only with this regressor variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#We selected the indexes we are interested in\n",
    "index1 = X.columns.get_loc('tokens_title')\n",
    "index2 = X.columns.get_loc('tokens_content')\n",
    "index3 = X.columns.get_loc('hrefs')\n",
    "index4 = X.columns.get_loc('imgs')\n",
    "index5 = X.columns.get_loc('videos')\n",
    "index6 = X.columns.get_loc('lifestyle')\n",
    "index7 = X.columns.get_loc('entertainment')\n",
    "index8 = X.columns.get_loc('bus')\n",
    "index9 = X.columns.get_loc('socmed')\n",
    "index10 = X.columns.get_loc('tech')\n",
    "index11 = X.columns.get_loc('world')\n",
    "index12 = X.columns.get_loc('self_reference_min_shares')\n",
    "index13 = X.columns.get_loc('self_reference_max_shares')\n",
    "index14 = X.columns.get_loc('self_reference_avg_shares')\n",
    "index15 = X.columns.get_loc('sentiment_polarity')\n",
    "index16 = X.columns.get_loc('global_rate_positive_words')\n",
    "index17 = X.columns.get_loc('global_rate_negative_words')\n",
    "index18 = X.columns.get_loc('title_sentiment_polarity')\n",
    "index19 = X.columns.get_loc('abs_title_subjectivity')\n",
    "index20 = X.columns.get_loc('abs_title_sentiment_polarity')\n",
    "index_len2 = len(X.columns)\n",
    "#We create a list with the indexes\n",
    "index_lst = [index1, index2, index3, index4, index5, index6, index7, index8, index9, index10, index11, index12, index13, index14, index15, index16, index17, index18, index19, index20]\n",
    "#We make them the columns of our X\n",
    "new_X= X.iloc[:, index_lst]\n",
    "print(new_X)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We split in train and test the new X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_X_train,new_X_test,y_train,y_test= train_test_split(new_X,y,train_size=0.78,random_state=42)\n",
    "print(\"new_X train shape: \",new_X_train.shape)\n",
    "print(\"new_X test shape: \",new_X_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scale the new X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index_len2 = len(new_X.columns)\n",
    "pipeline = ColumnTransformer([\n",
    "    ('num',RobustScaler(),list(range(0,index5+1))),\n",
    "    ('num1',RobustScaler(),list(range(index12, index_len2))),\n",
    "],remainder='passthrough')\n",
    "new_X_train = pipeline.fit_transform(new_X_train)\n",
    "new_X_test = pipeline.transform(new_X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can final try our best model on the  new X train:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svr_regression.fit(new_X_train,y_train)\n",
    "y_pred = svr_regression.predict(new_X_test)\n",
    "print(mean_squared_error(y_valid,y_pred,squared=False))\n",
    "print(mean_absolute_error(y_valid, y_pred))\n",
    "print(mean_absolute_percentage_error(y_valid, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "2.7.16 (default, Jun  5 2020, 22:59:21) \n[GCC 4.2.1 Compatible Apple LLVM 11.0.3 (clang-1103.0.29.20) (-macos10.15-objc-"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
